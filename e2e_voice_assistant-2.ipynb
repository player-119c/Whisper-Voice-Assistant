{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n"
      ],
      "metadata": {
        "id": "i2WRdIdCmz3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tUwoSTLbmTL4"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pydub\n",
        "!pip install git+https://github.com/snakers4/silero-vad.git\n",
        "\n",
        "!pip install Groq\n",
        "!pip install edge-tts\n",
        "!pip install nest_asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "pxtrMjhDmoiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RnBVQaOUmua1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install webrtcvad"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3xMXnm8s5DOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voice file to Text"
      ],
      "metadata": {
        "id": "J43r8bdHm5ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torchaudio\n",
        "import whisper\n",
        "\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "wav_path = \"/content/I%20am%20a%20big%20Brad%20Pitt.wav\"\n",
        "wav, sr = torchaudio.load(wav_path)\n",
        "print (sr)\n",
        "\n",
        "# Keeping Audio chanel count to 1\n",
        "if wav.shape[0] > 1:\n",
        "    wav = torch.mean(wav, dim=0, keepdim=True)\n",
        "\n",
        "wav = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(wav)\n",
        "# Applying noise reduction\n",
        "denoiser = torchaudio.transforms.Vad(sample_rate=16000)\n",
        "wav = denoiser(wav)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)\n",
        "get_speech_timestamps = utils[0]\n",
        "# kkeping sr to 16000 and vad threshold to 0.5 as specified in problem statement\n",
        "speech_timestamps = get_speech_timestamps(wav[0], vad_model, sampling_rate=16000, threshold=0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transcription = \"\"\n",
        "for i, segment in enumerate(speech_timestamps):\n",
        "    start, end = segment['start'], segment['end']\n",
        "    segment_wav = wav[:, start:end]\n",
        "\n",
        "\n",
        "    temp_segment_path = f\"temp_segment_{i}.wav\"\n",
        "    torchaudio.save(temp_segment_path, segment_wav, 16000)\n",
        "\n",
        "\n",
        "    result = model.transcribe(temp_segment_path)\n",
        "    transcription += result['text'] + \" \"\n",
        "\n",
        "\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "1GS12QgIm-D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to LLM\n"
      ],
      "metadata": {
        "id": "01L9Nqmpnpx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sending transcription to groq client using llama 3 llm for getting result\n",
        "# also hardcoded the query to keep the output short and relevant\n",
        "\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "\n",
        "client = Groq(\n",
        "    api_key=\"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\",\n",
        ")\n",
        "transcript= transcription + \"Please respond in short about 2-3 sentences without any bullet points  \"\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": transcript,\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "answer= chat_completion.choices[0].message.content\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "auLkqhu0nwTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM output to Voice"
      ],
      "metadata": {
        "id": "8bOmEe_PoCO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import edge_tts\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def text_to_speech(text, output_file, voice=\"en-US-JennyNeural\", rate=\"+0%\", pitch=\"0Hz\"):\n",
        "\n",
        "    communicate = edge_tts.Communicate(text, voice, rate=rate, pitch=pitch)\n",
        "    await communicate.save(output_file)\n",
        "\n",
        "# Define your text\n",
        "text = ( answer)\n",
        "# Choose your voice (e.g., en-US-JennyNeural for female, en-US-GuyNeural for male)\n",
        "voice = \"en-US-GuyNeural\"\n",
        "\n",
        "# Adjust the speed (rate) and pitch\n",
        "rate = \"+10%\"\n",
        "pitch = \"+1Hz\"\n",
        "\n",
        "\n",
        "output_file = \"output_test_test_2.mp3\"\n",
        "\n",
        "\n",
        "asyncio.run(text_to_speech(text, output_file, voice=voice, rate=rate, pitch=pitch))\n"
      ],
      "metadata": {
        "id": "Ve0aQM27oFim"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}